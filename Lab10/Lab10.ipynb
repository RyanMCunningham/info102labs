{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10\n",
    "\n",
    "# Wikispeedia Dataset\n",
    "\n",
    "There are many games a person can play using Wikipedia. `Wikispeedia` is a game that challenges players to find a path from a starting article to a destination article using only Wikipedia links. For example navigating from `Popcorn` to `Guitar` as quickly as possible would involve some creative thinking.\n",
    "We have 4,604 Wikipedia articles chosen for an online version of this game (`articles.txt`) and all the links between those pages (`links.txt`). These two files are included with this lab, but if you want to know more, you can download the original data here: `https://snap.stanford.edu/data/wikispeedia.html`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk\n",
    "\n",
    "The Python program below will read the `articles.txt` and `links.txt`. It will then pick a random Wikipedia article to start and follow random links `num_steps` times. We call this a random walk on the graph. Run this program a few times. Look for pages that show up frequently in the random walks. What would it mean for an article to show up a lot in these random walks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "Read in article names\n",
    "'''\n",
    "names=[]\n",
    "links={}\n",
    "for line in open(\"articles.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    names.append(line)\n",
    "    links[line]=[]\n",
    "    \n",
    "'''\n",
    "Read in links\n",
    "'''\n",
    "for line in open(\"links.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    splitline=line.split(\"\\t\")\n",
    "    if len(splitline)!=2:\n",
    "        continue\n",
    "    a,b=splitline\n",
    "    links[a].append(b)\n",
    "\n",
    "current=random.choice(names)\n",
    "print(current)\n",
    "\n",
    "num_steps=100\n",
    "for i in range(num_steps):\n",
    "    current=random.choice(links[current])\n",
    "    print(current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "Next, we will use the `Wikispeedia` dataset to explore the PageRank algorithm. The Python program below will read the files, compute the PageRank for each of the Wikipedia articles, and then write the result into a file called `ranking.txt`. Each line in the file has the name of the article and its PageRank. The file is sorted, so pages with the highest PageRank are displayed first. Try to relate what you observe to what you observed in the random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import networkx as nx\n",
    "\n",
    "'''\n",
    "Read in article names\n",
    "'''\n",
    "names=[]\n",
    "name2num={}\n",
    "for line in open(\"articles.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    name2num[line]=len(names)\n",
    "    names.append(line)\n",
    "    \n",
    "'''\n",
    "Read in links\n",
    "'''\n",
    "G=nx.DiGraph()\n",
    "for line in open(\"links.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    splitline=line.split(\"\\t\")\n",
    "    if len(splitline)!=2:\n",
    "        continue\n",
    "    a,b=splitline\n",
    "    i,j=name2num[a],name2num[b]\n",
    "    G.add_edge(i,j)\n",
    "    \n",
    "pr=nx.pagerank(G)\n",
    "\n",
    "sorted_pr=sorted(pr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "f=open(\"ranking.txt\",'w')\n",
    "for pagenum,rank in sorted_pr:\n",
    "    f.write(names[pagenum]+\"\\t\"+str(rank)+\"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the output of this program, click on the `File` in the top left and click `Open...` then choose `rankings.txt`. It's a big file, so be patient. If it's not there, don't forget to run the program first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Questions\n",
    "\n",
    "1. Looking over the random walks, what articles showed up frequently? Were there common themes in the articles you encountered most often? Why did the random walk visit these pages so frequently?\n",
    "2. What are some of the Wikipedia articles that have the highest PageRank in our dataset? Why might they have such a high PageRank?\n",
    "3. What are some of the Wikipedia articles that have the lowest PageRank in our dataset? Why might they have such a low PageRank?\n",
    "4. What is the total of all the PageRank values? Why is that the total?\n",
    "5. (CHALLENGE) Below, you'll find five SHA-1 hashes for passwords of length 2. The first character of each password is a lower-case letter and the second is a number (e.g. a1 or w7.) How many different passwords are possible? Can you crack the passwords?\n",
    "\n",
    "`a9dfb15be45a5f3128784c80c733f2cdee2f756a`\n",
    "\n",
    "`8b5eaccb28f2182b656fc1a2589dd64891df3a08`\n",
    "\n",
    "`68ee74f7d6afe0164fe0f1197aa9177c946d8834`\n",
    "\n",
    "`96d828acda84e1bc81c96b361f248dbe7758bace`\n",
    "\n",
    "`9865e9d9fefa7e998f1adaf68443075219c864fd`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
