{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12\n",
    "\n",
    "# Dataset\n",
    "Once again, we are using the Wikispeedia dataset, which consists of 4,604 Wikipedia articles chosen for an online version of the game (`articles.txt`) and all the links between those pages (`links.txt`). We’ve added 2 more files, which are the names of articles for US Presidents (`presidents.txt`) and names of articles for countries (`contries.txt`).\n",
    "\n",
    "# Jacard Similarity\n",
    "\n",
    "We learned about the Jaccard measure of similarity in class. This is an efficient and straightforward way to compare two sets. We will use Jaccard similarity today as a measure of how similar two Wikipedia pages are by comparing the links on each page. Two pages are similar if they contain many of the same links. Let’s call the set of links on the first page A and the set of links on the second page B. The Jaccard similarity is calculated by taking the ratio of the intersection and union of the two sets. $$J=\\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "The Python program below will compare the Jaccard similarity of two Wikipedia articles. Try a few different pairs of articles and observe how the measurement changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Read in list\n",
    "'''\n",
    "names=[]\n",
    "links={}\n",
    "for line in open(\"articles.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    names.append(line)\n",
    "    links[line]=[]\n",
    "\n",
    "'''\n",
    "Read in links\n",
    "'''\n",
    "for line in open(\"links.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    splitline=line.split(\"\\t\")\n",
    "    if len(splitline)!=2:\n",
    "        continue\n",
    "    a,b=splitline\n",
    "    if a in links:\n",
    "        links[a].append(b)\n",
    "\n",
    "article1=\"Gerald_Ford\"\n",
    "article2=\"United_States\"\n",
    "\n",
    "s1=set(links[article1])\n",
    "s2=set(links[article2])\n",
    "\n",
    "jaccard=float(len(s1&s2))/len(s1|s2)\n",
    "\n",
    "print(\"Jaccard similarity is: %f\\n\" % (jaccard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "We can use the Jaccard similarity to perform hierarchical clustering. One slight problem, though: clustering requires a *distance measure*. That means it should be small when the items are similar and large when they are different. Jaccard similarity has the *opposite* property. This is simple enough to solve, though. Jaccard similarity is a number between 0 and 1. Taking $$1-J(A,B)$$ results in a distance measure between A and B.\n",
    "\n",
    "The Python program below uses this method to calculate the distances between all the pairs of presidents in `presidents.txt`. It then plots a dendrogram. If it’s hard to read, you can open the file `dendrogram.pdf` for a better image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "'''\n",
    "Read in list\n",
    "'''\n",
    "names=[]\n",
    "links={}\n",
    "for line in open(\"presidents.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    names.append(line)\n",
    "    links[line]=[]\n",
    "\n",
    "'''\n",
    "Read in links\n",
    "'''\n",
    "for line in open(\"links.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    splitline=line.split(\"\\t\")\n",
    "    if len(splitline)!=2:\n",
    "        continue\n",
    "    a,b=splitline\n",
    "    if a in links:\n",
    "        links[a].append(b)\n",
    "\n",
    "distances=[]\n",
    "for i in range(len(names)):\n",
    "    for j in range(i+1,len(names)):\n",
    "        n1=names[i]\n",
    "        n2=names[j]\n",
    "        s1=set(links[n1])\n",
    "        s2=set(links[n2])\n",
    "        d=1.0-float(len(s1&s2))/len(s1|s2)\n",
    "        distances.append(d)\n",
    "\n",
    "link=linkage(distances,method='average')\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "dn = dendrogram(link,labels=names,color_threshold=.8)\n",
    "plt.savefig('dendrogram.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agglomeration method used is \"average\". Try using a few different methods and observe how the resulting clusters change. You'll do that by changing the line that looks like this in the code above:\n",
    "\n",
    "`link=linkage(distances,method='average')`\n",
    "\n",
    "Swap out 'average' with something else. You can find the different methods listed on this web page:\n",
    "\n",
    "`https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Clustering\n",
    "\n",
    "We can also use this method to cluster the countries dataset. You'll have to modify this line in the code above:\n",
    "\n",
    "`for line in open(\"presidents.txt\"):`\n",
    "\n",
    "This is a slightly larger dataset, so you’ll have to be patient and scroll around the dendrogram to interpret it. Again, try a few different clustering methods to see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the entire Wikipedia dataset is probably not feasible on this computer. It would take some time. There are 4,604 articles, which means there are 10,596,106 pairs of articles. We need to compute a distance between each pair of articles, which in practice requires about 2GB of RAM in Python. Clearly, this isn’t the best way to go about things. Fortunately, we’re learning about locality sensitive hashing in class.\n",
    "\n",
    "If we want a fast way of identifying articles that are similar? We learned how to estimate Jaccard similarity with minhash and how to find similar objects using locality sensitive hashing. The Python program below will perform this analysis on the entire Wikipedia dataset in a matter of seconds. The output will be stored in the file `minhash.csv`, which you can download and open in Excel. The pair of articles and their estimated Jaccard similarity will be displayed on each row.\n",
    "\n",
    "The program also displays some data about the performance of the program. How long it took to do various calculations and how many articles it compared. Try slightly changing the values of `b` (the number of bands) and `r` (the number of rows) to see how this changes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, operator\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "'''\n",
    "Read in list\n",
    "'''\n",
    "names=[]\n",
    "links={}\n",
    "for line in open(\"articles.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    names.append(line)\n",
    "    links[line]=[]\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Read in links\n",
    "\n",
    "'''\n",
    "for line in open(\"links.txt\"):\n",
    "    if line[0]==\"#\":\n",
    "        continue\n",
    "    line=line.strip()\n",
    "    splitline=line.split(\"\\t\")\n",
    "    if len(splitline)!=2:\n",
    "        continue\n",
    "    a,b=splitline\n",
    "    if a in links:\n",
    "        links[a].append(b)\n",
    "\n",
    "names=[n for n in names if len(links[n])>0]\n",
    "\n",
    "'''\n",
    "Hash function\n",
    "n is the number (which hash we want)\n",
    "s is the string we want to hash\n",
    "'''\n",
    "def qhash(n,s):\n",
    "    return hash(\"%d%s\" %(n,s))\n",
    "\n",
    "b=16        # bands\n",
    "r=4         # rows\n",
    "B=100000    # buckets\n",
    "m=b*r       # signature length\n",
    "\n",
    "# Compute minhash signatures\n",
    "start=timer()\n",
    "minhashes={}\n",
    "for name in names:\n",
    "    if len(links[name])==0:\n",
    "        continue\n",
    "    minhashes[name]=[]\n",
    "    for hash_num in range(m):\n",
    "        minhash=min([qhash(hash_num,link) for link in links[name]])\n",
    "        minhashes[name].append(minhash)\n",
    "end=timer()\n",
    "signature_time=end-start\n",
    "\n",
    "start=timer()\n",
    "distances={}\n",
    "for band in range(b):\n",
    "    buckets=[[] for i in range(B)]\n",
    "    for name in names:\n",
    "        bucket_num=0\n",
    "        for row in range(r):\n",
    "            sig=band*r+row\n",
    "            bucket_num^=minhashes[name][sig]\n",
    "        bucket_num=qhash(b,str(bucket_num))%B\n",
    "        buckets[bucket_num].append(name)\n",
    "    for bucket in buckets:\n",
    "        if len(bucket)<2:\n",
    "            continue\n",
    "        for n1,n2 in itertools.combinations(bucket,2):\n",
    "            if (n1,n2) not in distances:\n",
    "                score=float([x==y for x,y in zip(minhashes[n1],minhashes[n2])].count(True))/m\n",
    "                distances[n1,n2]=score\n",
    "end=timer()\n",
    "locality_time=end-start\n",
    "\n",
    "print(\"Bands: b=%d\" % b)\n",
    "print(\"Rows: r=%d\" %r)\n",
    "print(\"Signature length: m=%d\" % m)\n",
    "print(\"Time computing signatures: %f seconds\" % signature_time)\n",
    "print(\"Time performing locality sensitive hashing: %f seconds\" % locality_time)\n",
    "print(\"Pairs compared: %d\" % len(distances))\n",
    "print(\"Percent of pairs compared: %f%%.\" % (100*len(distances)/((len(names)*(len(names)-1))/2.)))\n",
    "\n",
    "sorted_distances=sorted(distances.items(), key=operator.itemgetter(1),reverse=True)\n",
    "f=open(\"minhash.csv\",'w')\n",
    "f.write(\"Article 1, Article 2, Signature Similarity\\n\")\n",
    "for pair,distance in sorted_distances:\n",
    "    f.write(\"%s,%s,%f\\n\" % (pair[0],pair[1],distance))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Report Questions\n",
    "1. In your experimenting with the Jaccard, what pages did you find that had a high similarity? What pages had low similarity? Does this match your intuition?\n",
    "2. Does the clustering produced for the U.S. Presidents match your knowledge of U.S. History? Can you explain what some of the clusters mean?\n",
    "3. What are some of the most similar pairs of presidents? Does this make intuitive sense to you?\n",
    "4. Does the clustering produced for the countries match your knowledge of geography? Can you explain what some of the clusters mean?\n",
    "5. What are some of the most similar pairs of countries? Does this make intuitive sense to you?\n",
    "6. How do the different agglomeration methods change the resulting clusters on our two datasets? Which method do you think results in the best clusters?\n",
    "7. How long does the minhash program take to run? What proportion of the documents did it compare? If you change the number of bands `b`, how does that change the results? If you change the number of rows `r`, how does that change the results?\n",
    "8. The file all_articles.pdf has a very large dendrogram of **all** the Wikipedia articles. Do you see any patterns in it? What do some of the large clusters correspond to?\n",
    "9. Looking at the full dendrogram of all the articles, did our minhash method do a good job of identifying the highly similar articles?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
